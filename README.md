# Jarvis TEC ğŸ¤–

Asistente inteligente con reconocimiento facial y procesamiento de voz para el TEC.

## ğŸ“ Estructura del Proyecto

```
/jarvis-tec/
  /frontend/                -> Persona B (Desarrollo Web)
    /public/
    /src/
      index.html           # Interfaz principal
      main.js              # LÃ³gica: cÃ¡mara + audio + parser + api-client
      api-client.js        # Cliente para comunicarse con backend
      styles.css           # Estilos
    package.json
    Dockerfile
    
  /backend/                 -> Persona A (ML & Backend)
    app.py                  # FastAPI - API principal
    /services/
      azure_face.py         # Servicio de Azure Face API
      model_runner.py       # Carga y ejecuta modelos entrenados
      stt_service.py        # Speech-to-Text (Azure/Google/Whisper)
    /models/                # Modelos entrenados (.joblib, .pkl)
    requirements.txt
    Dockerfile
    .env.example
    
  /notebooks/               -> Persona A (Entrenamiento)
    01_data_exploration.ipynb
    02_model_training.ipynb
    
  docker-compose.yml
  .gitignore
  README.md
```

## ğŸš€ Inicio RÃ¡pido

### Requisitos Previos

- Python 3.10+
- Node.js 18+ (para frontend)
- Docker y Docker Compose (opcional)
- Azure Cognitive Services (Face API y Speech Services)

### ConfiguraciÃ³n Backend (Persona A)

1. **Navegar al directorio backend:**
   ```bash
   cd backend
   ```

2. **Crear entorno virtual:**
   ```bash
   python -m venv venv
   venv\Scripts\activate  # Windows
   ```

3. **Instalar dependencias:**
   ```bash
   pip install -r requirements.txt
   ```

4. **Configurar variables de entorno:**
   ```bash
   cp .env.example .env
   # Editar .env con tus credenciales de Azure
   ```

5. **Ejecutar servidor:**
   ```bash
   python app.py
   ```

   El backend estarÃ¡ disponible en: `http://localhost:8000`

### ConfiguraciÃ³n Frontend (Persona B)

1. **Navegar al directorio frontend:**
   ```bash
   cd frontend
   ```

2. **Instalar dependencias:**
   ```bash
   npm install
   ```

3. **Ejecutar servidor de desarrollo:**
   ```bash
   npm run dev
   ```

   El frontend estarÃ¡ disponible en: `http://localhost:3000`

### Usar Docker Compose (Ambos)

```bash
docker-compose up --build
```

## ğŸ“‹ Tareas por Persona

### Persona A - Backend & Machine Learning

- âœ… Configurar Azure Face API y Speech Services
- âœ… Implementar endpoints FastAPI
- âœ… Entrenar modelos de ML en notebooks
- âœ… Implementar `model_runner.py` para cargar modelos
- âœ… Configurar servicio STT (Speech-to-Text)
- ğŸ“ Crear y entrenar modelos personalizados
- ğŸ“ Optimizar rendimiento de reconocimiento

### Persona B - Frontend

- âœ… DiseÃ±ar interfaz HTML/CSS
- âœ… Implementar captura de cÃ¡mara y audio
- âœ… Crear cliente API (`api-client.js`)
- âœ… Implementar visualizaciÃ³n de resultados
- ğŸ“ Mejorar UX/UI
- ğŸ“ Agregar notificaciones y feedback visual

## ğŸ”§ ConfiguraciÃ³n de Azure

### Azure Face API

1. Crear recurso Face en Azure Portal
2. Obtener Key y Endpoint
3. Configurar en `.env`:
   ```
   AZURE_FACE_KEY=tu_key_aqui
   AZURE_FACE_ENDPOINT=https://tu-recurso.cognitiveservices.azure.com/
   ```

### Azure Speech Services

1. Crear recurso Speech en Azure Portal
2. Obtener Key y Region
3. Configurar en `.env`:
   ```
   AZURE_SPEECH_KEY=tu_key_aqui
   AZURE_SERVICE_REGION=tu_region_aqui
   ```

## ğŸ“š Endpoints de la API

### `POST /api/transcribe`
Transcribe audio a texto.
- **Input:** Archivo de audio (webm)
- **Output:** `{ "transcription": "texto" }`

### `POST /api/recognize-face`
Reconoce rostro en imagen.
- **Input:** Imagen (jpg/png)
- **Output:** `{ "identified": true/false, "name": "...", ... }`

### `POST /api/process`
Procesa comando de texto con modelo ML.
- **Input:** `{ "text": "comando" }`
- **Output:** `{ "response": "respuesta" }`

### `GET /api/health`
Verifica estado del sistema.
- **Output:** `{ "status": "healthy", "services": {...} }`

## ğŸ§ª Notebooks de Entrenamiento

Los notebooks en `/notebooks/` contienen:

1. **01_data_exploration.ipynb**: AnÃ¡lisis exploratorio de datos
2. **02_model_training.ipynb**: Entrenamiento de modelos

Para usar los notebooks:
```bash
cd notebooks
jupyter notebook
```

## ğŸ³ Docker

### Construir imÃ¡genes:
```bash
docker-compose build
```

### Ejecutar servicios:
```bash
docker-compose up
```

### Detener servicios:
```bash
docker-compose down
```

## ğŸ¤ ColaboraciÃ³n

- **Persona A**: Enfocado en backend, ML y servicios de Azure
- **Persona B**: Enfocado en frontend, UX/UI y integraciÃ³n de API

### Workflow sugerido:

1. Persona A desarrolla y prueba endpoints
2. Persona B consume endpoints desde frontend
3. Ambos prueban integraciÃ³n completa
4. IteraciÃ³n y mejoras continuas

## ğŸ“ Notas

- Los modelos entrenados (`.joblib`, `.pkl`) no se suben a Git (ver `.gitignore`)
- Compartir modelos vÃ­a Google Drive o similar
- Las credenciales de Azure deben mantenerse en `.env` (no subir a Git)

## ğŸ”— Enlaces Ãštiles

- [FastAPI Docs](https://fastapi.tiangolo.com/)
- [Azure Face API Docs](https://docs.microsoft.com/en-us/azure/cognitive-services/face/)
- [Azure Speech Services Docs](https://docs.microsoft.com/en-us/azure/cognitive-services/speech-service/)
- [MDN Web APIs](https://developer.mozilla.org/en-US/docs/Web/API)

## ğŸ“„ Licencia

MIT License - Ver archivo LICENSE para mÃ¡s detalles.

---

**Desarrollado por el equipo Jarvis TEC** ğŸš€